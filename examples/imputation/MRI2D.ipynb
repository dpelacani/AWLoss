{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = '/home/oab18/Projects/'\n",
    "path_data = '/home/oab18/Desktop/HCP Young Adult Database/' # \"/home/dp4018/data/ultrasound-data/Ultrasound-MRI-sagittal/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "sys.path.append(path_root+'/AWLoss/awloss/')\n",
    "sys.path.append(path_root+'/AWLoss/examples')\n",
    "sys.path.append(path_root+'/AWLoss/')\n",
    "\n",
    "%load_ext autoreload\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import Compose, Resize, Lambda, Normalize\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from monai.networks.nets import UNet\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from torch.nn.functional import interpolate\n",
    "from awloss import AWLoss\n",
    "\n",
    "from train_utils import *\n",
    "\n",
    "%autoreload 2\n",
    "from networks import *\n",
    "from datasets import MaskedUltrasoundDataset2D\n",
    "from landscape import *\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as clt\n",
    "import progressbar\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed, clear cache and enable anomaly detection (for debugging)\n",
    "set_seed(42)\n",
    "torch.cuda.empty_cache()\n",
    "torch.autograd.set_detect_anomaly(True)                     \n",
    "device=set_device(\"cuda\", 0)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.path.abspath(\"/media/dekape/HDD/Ultrasound-MRI-sagittal/\")\n",
    "size = 320\n",
    "path = os.path.abspath(path_data)\n",
    "train_transform = Compose([\n",
    "                    Resize(size),\n",
    "                    Lambda(lambda x: x / x.abs().max()),\n",
    "                    Lambda(lambda x: clip_outliers(x, \"outer\")),\n",
    "                    Lambda(lambda x: scale2range(x, [0., 1.])),\n",
    "                    ])\n",
    "\n",
    "mask = create_mask((size,size), (0,3), (0,1))\n",
    "\n",
    "ds = MaskedUltrasoundDataset2D(path, \n",
    "                                    mode=\"mri\",\n",
    "                                    transform=train_transform,\n",
    "                                    mask=mask,\n",
    "                                    maxsamples=None)\n",
    "print(ds, \"\\n\")\n",
    "print(ds.__len__())\n",
    "\n",
    "print(ds.info(nsamples=30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.2\n",
    "i = int(len(ds)*valid_ratio)\n",
    "\n",
    "idxs = np.arange(0, len(ds), 1)\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "train_idxs, valid_idxs = idxs[:-i], idxs[-i:]\n",
    "trainds, validds = Subset(ds, train_idxs), Subset(ds, valid_idxs)\n",
    "\n",
    "print(len(trainds), len(validds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(min(len(trainds), 10)):\n",
    "    data += list((trainds[i][1].flatten().detach().cpu().numpy()))\n",
    "plt.title(\"Train Dataset Value\")\n",
    "plt.hist(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(nc=64):\n",
    "    set_seed(42)\n",
    "    channels = (16, 32, 64)\n",
    "    model =  UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=nc,\n",
    "    out_channels=nc,\n",
    "    channels=channels,\n",
    "    strides=tuple([2 for i in range(len(channels))]), \n",
    "    num_res_units=3,\n",
    "    act=\"mish\")\n",
    "    model = nn.DataParallel(model) \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss, train_loader, valid_loader=None, nepochs=150, \n",
    "                log_frequency=10, sample_input=None, sample_target=None, device=\"cpu\", \n",
    "                exp_name=\"\", save=True, scheduler=None):\n",
    "                \n",
    "    print(\"\\n\\nTraining started ...\")\n",
    "    try:\n",
    "        all_train_losses, all_valid_losses = [], []\n",
    "        with progressbar.ProgressBar(max_value=nepochs) as bar:    \n",
    "            for epoch in range(nepochs):\n",
    "                # Train and validate epoch\n",
    "                train_loss = train(model, train_loader, optimizer, loss, scheduler, device)\n",
    "                all_train_losses.append(train_loss.item())\n",
    "                if valid_loader:\n",
    "                    valid_loss = validate(model, valid_loader, loss, device)\n",
    "                    all_valid_losses.append(valid_loss.item())\n",
    "                \n",
    "                bar.update(epoch)\n",
    "                \n",
    "                # Logging\n",
    "                log = {\"epoch\": epoch, \"train_loss\": train_loss.item()}\n",
    "                if valid_loader:\n",
    "                    log.update({\"valid_loss\": valid_loss.item()})\n",
    "                    \n",
    "                if (epoch % log_frequency == 0 or epoch==nepochs-1):\n",
    "                    print(\"\\n\", log)\n",
    "\n",
    "                    if valid_loader:\n",
    "                        model.eval()\n",
    "                        X, target = next(iter(valid_loader))\n",
    "                        X, target = X[:train_loader.batch_size], target[:train_loader.batch_size]\n",
    "                        recon = torch.sigmoid(model(X))\n",
    "\n",
    "                        fig, axs = plt.subplots(4, 1, figsize=(10*train_loader.batch_size, 15))\n",
    "                        axs[0].imshow(make_grid(X, pad_value=0, padding=2, vmin=0, vmax=1).cpu().data[0], cmap='Greys_r')\n",
    "                        axs[1].imshow(make_grid(recon, pad_value=0, padding=2, vmin=0, vmax=1).cpu().data[0], cmap='Greys_r')\n",
    "                        axs[2].imshow(make_grid(target, pad_value=0, padding=2, vmin=0, vmax=1).cpu().data[0], cmap='Greys_r')\n",
    "                        try:\n",
    "                            loss(X, target)\n",
    "                            v = loss.filters\n",
    "                        except:\n",
    "                            v = torch.zeros_like(X)\n",
    "                        axs[3].imshow(make_grid(v, pad_value=0, padding=2, vmin=-0.1, vmax=0.1).cpu().data[0], cmap='seismic')\n",
    "                        plt.show()\n",
    "\n",
    "                    if sample_input is not None:\n",
    "                        idx = int(sample_input.shape[0]/2)\n",
    "                        samples = {\"Input idx %g\"%idx: sample_input[idx]}\n",
    "\n",
    "                        # Model forward pass\n",
    "                        model.eval()\n",
    "                        X = sample_input.unsqueeze(0).to(device)\n",
    "                        recon = torch.sigmoid(model(X))[0]\n",
    "                        samples.update({\"Reconstruction idx %g\"%idx: recon[idx].cpu().detach().numpy()})\n",
    "\n",
    "                        # If testing sample provided\n",
    "                        if sample_target is not None:\n",
    "                            samples.update({\"Target idx %g\"%idx: sample_target[idx]})\n",
    "\n",
    "                            # Loss evaluation and filters\n",
    "                            f = loss(recon.unsqueeze(0).to(device), sample_target.unsqueeze(0).to(device))\n",
    "                            try:\n",
    "                                v, T = loss.filters[0], loss.T\n",
    "                            except:\n",
    "                                try:\n",
    "                                    loss_list = [str(l) for l in loss.losses]\n",
    "                                    awloss = loss.losses[loss_list.index(\"AWLoss()\")]\n",
    "                                    v, T = awloss.filters[0], awloss.T\n",
    "                                except:\n",
    "                                    v, T = torch.tensor([0.]), torch.tensor([0.])\n",
    "                            print(\" argidx T, v: \",torch.argmin(torch.abs(T)).item(), torch.argmax(torch.abs(v)).item())\n",
    "                \n",
    "                    samples_fig = plot_samples(samples)\n",
    "                    losses_fig = plot_losses(losses={\"train\": all_train_losses, \"valid\":all_valid_losses},\n",
    "                                filters={\"Weiner Filter\": v.flatten().cpu().detach().numpy(), \"Penalty\": T.flatten().cpu().detach().numpy()})\n",
    "        raise(KeyboardInterrupt)\n",
    "    except KeyboardInterrupt:                                   \n",
    "        if save:\n",
    "            objs = { \"mask\": train_loader.dataset.dataset.mask,\n",
    "                \"train_loader\":train_loader,\n",
    "                \"valid_loader\":valid_loader,\n",
    "                \"x_sample\": x_sample,\n",
    "                \"y_sample\": y_sample,\n",
    "                \"recon\": recon,\n",
    "\n",
    "                \"model\": model,\n",
    "                \"optim\": optimizer,\n",
    "                \"loss\": loss,\n",
    "                \"train_losses\": all_train_losses,\n",
    "                \"vald_losses\": all_valid_losses,\n",
    "                \"penalty\": T,\n",
    "                }\n",
    "\n",
    "            summary = { \"data_mode\": train_loader.dataset.dataset.mode,\n",
    "                        \"interpolation_model\": \"UNet\",\n",
    "                        \"loss\": str(loss),\n",
    "                        \"img_size\": x_sample.numpy().shape,\n",
    "                        \"device\":device,\n",
    "                        \"nepochs\": nepochs,\n",
    "                        \"current_epoch\":epoch, \n",
    "                        \"learning_rate\":optimizer.defaults[\"lr\"],\n",
    "                        \"batch_size\":train_loader.batch_size,\n",
    "                        \"ntrain\": len(train_loader.dataset),}\n",
    "            try:\n",
    "                summary[\"nvalid\"]= len(valid_loader.dataset)\n",
    "            except:\n",
    "                summary[\"nvalid\"]= 0\n",
    "            try:\n",
    "                summary[\"aw_filter_dim\"] = loss.filter_dim,\n",
    "                summary[\"aw_epsilon\"] =  loss.epsilon,\n",
    "                summary[ \"aw_std\"] = loss.std\n",
    "            except:\n",
    "                summary[\"aw_filter_dim\"] = None\n",
    "                summary[\"aw_epsilon\"] =  None\n",
    "                summary[ \"aw_std\"] = None\n",
    "\n",
    "            figs = {\"losses\":losses_fig, \"samples\":samples_fig}\n",
    "            save_exp(objs=objs, figs=figs, summary=summary, overwrite=False)            \n",
    "    return None\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training static parameters and hyperparameters\n",
    "nepochs=300                        \n",
    "learning_rate=1e-2\n",
    "batch_size=32                                        \n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(trainds,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=4)\n",
    "valid_loader = DataLoader(validds,\n",
    "                        batch_size=1000,\n",
    "                        shuffle=False,\n",
    "                        num_workers=4)\n",
    "\n",
    "\n",
    "# Sample for visualisation\n",
    "x_sample, y_sample = validds[0]\n",
    "f = (nepochs//2) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = next(iter(train_loader))\n",
    "fig, axs = plt.subplots(2, 1, figsize=(4*batch_size,7))\n",
    "axs[0].imshow(make_grid(train_batch[0], pad_value=0, padding=3).data[0], cmap='Greys_r')\n",
    "axs[0].set_title(\"Masked Input\")\n",
    "axs[1].imshow(make_grid(train_batch[1], pad_value=0, padding=3).data[0], cmap='Greys_r')\n",
    "axs[1].set_title(\"Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train AWLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian2D(mesh):\n",
    "    alpha, beta = -0.2, 1.5\n",
    "    xx, yy = mesh[:,:,0], mesh[:,:,1]\n",
    "    x = torch.sqrt(xx**2 + yy**2) \n",
    "    T = 1 - torch.exp(-torch.abs(x) ** alpha) ** beta\n",
    "    T = scale2range(T, [0.05, 1.])\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awmodel = make_model(nc=x_sample.shape[0])\n",
    "awoptim = torch.optim.Adam(awmodel.parameters(), lr=learning_rate)\n",
    "\n",
    "awloss     = AWLoss(filter_dim=2, method=\"fft\", reduction=\"mean\", store_filters=\"unorm\", \n",
    "                    epsilon=250., filter_scale=2, penalty_function=laplacian2D)\n",
    "\n",
    "\n",
    "train_model(awmodel, awoptim, awloss, train_loader, valid_loader=valid_loader, nepochs=nepochs, log_frequency=100, \n",
    "            sample_input=x_sample, sample_target=y_sample, device=device, save=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb39db07ac5cbc04a89cfdc5b09019c70d8dde2a36a44e5f300e1e731ec12760"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
